---
title: "Predictive modeling of claims status"
author: 'Edwin Yang, Carter Kulm, James San, Liuqian Bao'
date: today
---

### Abstract

Provide a 3-5 sentence summary of your work on the primary task. Indicate what input data was used, what method was used for binary class predictions, what method was used for multiclass predictions, and what estimated accuracies were achieved.

> *Header and paragraph content was scraped from the raw webpages and processed into term frequencies of word tokens. For binary classification, a six-layer recurrent neural network yielded an estimated 76.6% accuracy. And for the multiclass classification, the same RNN gave 61.9% accuracy.*

### Preprocessing

In one paragraph lay out your preprocessing pipeline. No need to provide exact step-by-step detail; just give an overview of the main components:

-   what text content was extracted from HTML

-   how text was cleaned

-   how cleaned text was represented quantitatively

> *In the original preprocessing code, it previously included only paragraph data. We added header content to include more data, since we expect header data to be more informative than paragraph data. How we cleaned the text was removing all of the punctuation, removed non-letter symbols, numbers, urls, emails, and lowercased all words. The cleaned text are words that appear in the data from header and paragraph content. We would apply a preprocessing layer `layer_text_vectorization` from the `keras` package which maps these words to their tokenized index counterparts, and subsequently applied the `adapt` function which helped to supply our RNN with the learned vocabulary.*

### Methods

Describe your final predictive models. Include one paragraph with details on the binary classification approach, and one on the multiclass approach. Include for each:

-   what ML/statistical method was used

-   model specification and hyperparameter selection

-   training method

> ***Binary Classification**: We used a RNN model as our method. The model uses an embedding layer to map the learned vocabulary into a learnable vector space for the RNN. This layer takes in the 10000 tokens specified by the preprocessing layer, each with a sequence length of 100, and outputs 128 vectors. We follow this with an LSTM layer with 64 units that captures temporal dependencies in the data, and gives the RNN its "recurrent" nature. It includes dropout layers (0.5 and 0.3) that regularize node inputs that help to reduce overfitting. Two dense layers are included, with the final dense layer using a sigmoid activation function for binary classification. The final layer outputs one unit, which corresponds to the prediction of class. The model is compiled with binary crossentropy as the loss function, optimized using the Adam optimizer, and evaluates performance with binary accuracy. Training is conducted over 11 epochs with a batch size of 32, using 20% of the training data for validation.*
>
> ***Multiclass Classification**: Our multiclassification model is almost exactly the same as our binary classification. The only differences come near the end of the network, where we must output not one unit from our last layer but the amount of units that correspond to the number of classification labels (5, in our case). Furthermore, we use a softmax activation function on this last layer rather than the sigmoid that was used for the binary model. *

### Results

Indicate the predictive accuracy of the binary classifications and the multiclass classifications. Provide a table for each, and report sensitivity, specificity, and accuracy.[^1]

[^1]: Read [this article](https://yardstick.tidymodels.org/articles/multiclass.html) on multiclass averaging.

|     |
|-----|

| Binary Model (RNN) |       |
|--------------------|-------|
| sensitivity        | 0.842 |
| specificity        | 0.703 |
| accuracy           | 0.766 |

| Multiclass Model (RNN) |       |
|------------------------|-------|
| sensitivity            | 0.538 |
| specificity            | 0.892 |
| accuracy               | 0.619 |
