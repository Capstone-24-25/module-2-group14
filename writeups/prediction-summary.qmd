---
title: "Predictive modeling of claims status"
author: 'Edwin Yang, Carter Kulm, James San, Liuqian Bao'
date: today
---

### Abstract

Provide a 3-5 sentence summary of your work on the primary task. Indicate what input data was used, what method was used for binary class predictions, what method was used for multiclass predictions, and what estimated accuracies were achieved.

> *Header and paragraph content was scraped from the raw webpages and processed into term frequencies of word tokens. For binary classification, a four-layer neural network yielded an estimated 81.3% accuracy; for multiclass classification, a support vector machine gave 78% accuracy.*

### Preprocessing

In one paragraph lay out your preprocessing pipeline. No need to provide exact step-by-step detail; just give an overview of the main components:

-   what text content was extracted from HTML

-   how text was cleaned

-   how cleaned text was represented quantitatively

> *In the original preprocessing code, it previously included only paragraph data. We added header content to include more data, since we expect header data to be more informative than paragraph data. How we cleaned the text was removing all of the punctuation, removed non-letter symbols, and lowercased all words. The cleaned text are words that appear in the data from header and paragraph content. We tokenized each word to be represented quantitatively, and for the ngrams we tokenized two consecutive words as a token, which creates a bigram.*

### Methods

Describe your final predictive models. Include one paragraph with details on the binary classification approach, and one on the multiclass approach. Include for each:

-   what ML/statistical method was used

-   model specification and hyperparameter selection

-   training method

> ***Binary Classification**: We used a RNN model as our method. The model uses an embedding layer to transform input sequences into dense vector representations, followed by an LSTM layer with 64 units to capture temporal dependencies in the data. It includes dropout layers (0.5 and 0.3) for regularization to reduce overfitting. Two dense layers are included, with the final dense layer using a sigmoid activation function for binary classification. The model is compiled with binary crossentropy as the loss function, optimized using the Adam optimizer, and evaluates performance with binary accuracy. Training is conducted over 11 epochs with a batch size of 32, using 20% of the training data for validation.*
>
> ***Multiclass Classification**: We also used RNN for the multiclass setting. The model uses an embedding layer that converts input sequences into dense vector representations. A Long Short-Term Memory (LSTM) layer with 64 units processes these sequences to capture temporal dependencies, followed by dropout layers (with rates of 0.5 and 0.3) to reduce overfitting. A dense layer with 32 units is added, and the final dense layer uses a softmax activation function to output probabilities across the number of classes, equal to the unique labels in the training data. The model is compiled with sparse categorical crossentropy as the loss function and evaluates performance using sparse categorical accuracy. It is trained over 11 epochs with a batch size of 32, using 20% of the training data for validation.*

### Results

Indicate the predictive accuracy of the binary classifications and the multiclass classifications. Provide a table for each, and report sensitivity, specificity, and accuracy.[^1]

[^1]: Read [this article](https://yardstick.tidymodels.org/articles/multiclass.html) on multiclass averaging.

| Binary Model (RNN) |     |
|--------------------|-----|
| sensitivity        |     |
| specificity        |     |
| accuracy           |     |

| Multiclass Model (RNN) |     |
|------------------------|-----|
| sensitivity            |     |
| specificity            |     |
| accuracy               |     |
